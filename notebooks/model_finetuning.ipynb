{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers peft datasets torch torchvision open-clip-torch decord moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "# Setup\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-11B\"\n",
    "OUTPUT_DIR = \"./llama-movie-recommender-multimodal\"\n",
    "DEVICE_MAP = \"auto\"\n",
    "IMAGE_MODEL = \"openai/clip-vit-base-patch32\"\n",
    "QUANTIZATION_CONFIG = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "}\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=DEVICE_MAP,\n",
    "    **QUANTIZATION_CONFIG,\n",
    ")\n",
    "\n",
    "# Load CLIP model for image and video features\n",
    "clip_model, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B/32\", pretrained=\"openai\"\n",
    ")\n",
    "clip_model.eval()\n",
    "\n",
    "# Video preprocessing\n",
    "video_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define functions for feature extraction\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_tensor).numpy()\n",
    "    return image_features\n",
    "\n",
    "def extract_trailer_features(trailer_path):\n",
    "    vr = VideoReader(trailer_path, ctx=cpu(0))\n",
    "    frame_features = []\n",
    "    for frame in vr:\n",
    "        image = video_preprocess(frame.asnumpy())\n",
    "        image = image.unsqueeze(0).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.encode_image(image)\n",
    "            frame_features.append(features.cpu().numpy())\n",
    "    video_features = np.mean(frame_features, axis=0)\n",
    "    return video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset and add multimodal features\n",
    "print(\"Loading Dataset...\")\n",
    "dataset = load_dataset(\"movielens\", split=\"train\")\n",
    "\n",
    "# Add features to the dataset\n",
    "def add_multimodal_features(row):\n",
    "    row[\"poster_features\"] = extract_image_features(row[\"poster_path\"])\n",
    "    row[\"trailer_features\"] = extract_trailer_features(row[\"trailer_path\"])\n",
    "    row[\"movie_features\"] = np.concatenate([row[\"poster_features\"], row[\"trailer_features\"]])\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(add_multimodal_features, num_proc=4)\n",
    "\n",
    "# Format data for fine-tuning\n",
    "def format_multimodal(row):\n",
    "    instruction = \"You are a multimodal movie recommender system. Suggest movies based on user preferences, posters, and trailers.\"\n",
    "    user_input = f\"User Preferences: {row['user_preferences']}\"\n",
    "    movie_features = f\"Movie Features: {row['movie_features']}\"\n",
    "    recommended_movies = f\"Recommended Movies: {row['recommended_movies']}\"\n",
    "    row[\"text\"] = f\"{instruction}\\n{user_input}\\n{movie_features}\\n{recommended_movies}\"\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(format_multimodal, num_proc=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configure LoRA\n",
    "print(\"Setting up LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=10,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start training\n",
    "print(\"Starting Training...\")\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving Model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {OUTPUT_DIR}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model turned out to be overfitted . Fixing IT !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
